#!/usr/bin/env python3
"""
ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ4: é«˜åº¦ãªå˜èªè§£æã‚¢ãƒ—ãƒª

æ©Ÿæ¢°å­¦ç¿’é¢¨ã®æ©Ÿèƒ½ã‚’å«ã‚€é«˜åº¦ãªãƒ†ã‚­ã‚¹ãƒˆè§£æã‚¢ãƒ—ãƒªã§ã™ã€‚
ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰é™¤å»ã€N-gramè§£æã€æ„Ÿæƒ…ã‚¹ã‚³ã‚¢ãªã©ã‚’å®Ÿè£…ã—ã¦ã„ã¾ã™ã€‚

å­¦ç¿’ãƒã‚¤ãƒ³ãƒˆ:
- æ¨™æº–ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®é«˜åº¦ãªæ´»ç”¨
- ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®å®Ÿè£…
- ãƒ‡ãƒ¼ã‚¿å¯è¦–åŒ–ã®åŸºç¤
- ã‚ˆã‚Šè¤‡é›‘ãªãƒ†ã‚­ã‚¹ãƒˆå‡¦ç†

å¯¾å¿œç« : basics/12_external_libraries.pyå®Œäº†å¾Œ
"""

import re
import math
import json
from collections import Counter, defaultdict
from pathlib import Path

class AdvancedWordAnalyzer:
    """é«˜åº¦ãªå˜èªè§£æã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        """ã‚¢ãƒŠãƒ©ã‚¤ã‚¶ãƒ¼ã‚’åˆæœŸåŒ–"""
        self.text = ""
        self.sentences = []
        self.words = []
        self.word_count = {}
        self.stats = {}
        
        # ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰ï¼ˆé™¤å¤–ã™ã‚‹ä¸€èˆ¬çš„ãªå˜èªï¼‰
        self.stopwords = {
            'english': {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'is', 'are', 'was', 'were', 'be', 'been', 'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could', 'should', 'this', 'that', 'these', 'those'},
            'japanese': {'ã¯', 'ãŒ', 'ã‚’', 'ã«', 'ã§', 'ã¨', 'ã®', 'ã ', 'ã§ã‚ã‚‹', 'ã§ã™', 'ã¾ã™', 'ã—ãŸ', 'ã™ã‚‹', 'ã“ã®', 'ãã®', 'ã‚ã®', 'ã“ã‚Œ', 'ãã‚Œ', 'ã‚ã‚Œ'}
        }
        
        # æ„Ÿæƒ…è¾æ›¸ï¼ˆç°¡æ˜“ç‰ˆï¼‰
        self.sentiment_dict = {
            'positive': {'good', 'great', 'excellent', 'amazing', 'wonderful', 'fantastic', 'love', 'like', 'happy', 'joy', 'beautiful', 'perfect', 'awesome', 'brilliant'},
            'negative': {'bad', 'terrible', 'awful', 'horrible', 'hate', 'dislike', 'sad', 'angry', 'ugly', 'worst', 'pain', 'difficult', 'problem', 'issue'}
        }
    
    def load_text_from_file(self, filename):
        """ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’èª­ã¿è¾¼ã¿"""
        try:
            with open(filename, 'r', encoding='utf-8') as f:
                self.text = f.read()
            print(f"âœ… ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ: {filename}")
            return True
        except Exception as e:
            print(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def preprocess_text(self, remove_stopwords=True, language='english'):
        """é«˜åº¦ãªãƒ†ã‚­ã‚¹ãƒˆå‰å‡¦ç†"""
        if not self.text:
            return False
        
        # æ–‡å˜ä½ã§åˆ†å‰²
        self.sentences = re.split(r'[.!?ã€‚ï¼ï¼Ÿ]', self.text)
        self.sentences = [s.strip() for s in self.sentences if s.strip()]
        
        # ãƒ†ã‚­ã‚¹ãƒˆã®æ­£è¦åŒ–
        text_lower = self.text.lower()
        
        # ç‰¹æ®Šæ–‡å­—ã®å‡¦ç†
        text_processed = re.sub(r'[^\w\s]', ' ', text_lower)
        text_processed = re.sub(r'\s+', ' ', text_processed)
        
        # å˜èªåˆ†å‰²
        words = [word.strip() for word in text_processed.split() if word.strip()]
        
        # ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰é™¤å»
        if remove_stopwords and language in self.stopwords:
            words = [word for word in words if word not in self.stopwords[language]]
        
        # çŸ­ã™ãã‚‹å˜èªã‚’é™¤å»ï¼ˆ1æ–‡å­—ä»¥ä¸‹ï¼‰
        words = [word for word in words if len(word) > 1]
        
        self.words = words
        print(f"âœ… é«˜åº¦ãªå‰å‡¦ç†å®Œäº†: {len(self.words)}å€‹ã®å˜èªã‚’æŠ½å‡º")
        return True
    
    def calculate_advanced_statistics(self):
        """é«˜åº¦ãªçµ±è¨ˆæƒ…å ±ã‚’è¨ˆç®—"""
        if not self.words:
            return False
        
        self.word_count = Counter(self.words)
        
        # åŸºæœ¬çµ±è¨ˆ
        total_words = len(self.words)
        unique_words = len(self.word_count)
        
        # èªå½™ã®è±Šå¯Œã•
        ttr = unique_words / total_words if total_words > 0 else 0
        
        # å˜èªé•·çµ±è¨ˆ
        word_lengths = [len(word) for word in self.words]
        avg_word_length = sum(word_lengths) / len(word_lengths)
        
        # æ–‡ã®çµ±è¨ˆ
        sentence_lengths = [len(sentence.split()) for sentence in self.sentences]
        avg_sentence_length = sum(sentence_lengths) / len(sentence_lengths) if sentence_lengths else 0
        
        # èª­ã¿ã‚„ã™ã•ã‚¹ã‚³ã‚¢ï¼ˆç°¡æ˜“ç‰ˆï¼‰
        readability_score = self._calculate_readability()
        
        # æ„Ÿæƒ…ã‚¹ã‚³ã‚¢
        sentiment_score = self._calculate_sentiment()
        
        self.stats = {
            'total_words': total_words,
            'unique_words': unique_words,
            'total_sentences': len(self.sentences),
            'type_token_ratio': ttr,
            'average_word_length': avg_word_length,
            'average_sentence_length': avg_sentence_length,
            'readability_score': readability_score,
            'sentiment_score': sentiment_score
        }
        
        return True
    
    def _calculate_readability(self):
        """èª­ã¿ã‚„ã™ã•ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—ï¼ˆFlesch Reading Easeé¢¨ï¼‰"""
        if not self.sentences or not self.words:
            return 0
        
        total_sentences = len(self.sentences)
        total_words = len(self.words)
        total_syllables = sum(self._count_syllables(word) for word in self.words)
        
        if total_sentences == 0 or total_words == 0:
            return 0
        
        # ç°¡æ˜“Flesch Reading Easeé¢¨ã®è¨ˆç®—
        score = 206.835 - 1.015 * (total_words / total_sentences) - 84.6 * (total_syllables / total_words)
        return max(0, min(100, score))
    
    def _count_syllables(self, word):
        """éŸ³ç¯€æ•°ã‚’æ¨å®šï¼ˆè‹±èªã®å ´åˆï¼‰"""
        word = word.lower()
        vowels = 'aeiouy'
        syllable_count = 0
        prev_was_vowel = False
        
        for char in word:
            is_vowel = char in vowels
            if is_vowel and not prev_was_vowel:
                syllable_count += 1
            prev_was_vowel = is_vowel
        
        # æœ€ä½1éŸ³ç¯€
        return max(1, syllable_count)
    
    def _calculate_sentiment(self):
        """æ„Ÿæƒ…ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—"""
        positive_count = 0
        negative_count = 0
        
        for word in self.words:
            if word in self.sentiment_dict['positive']:
                positive_count += 1
            elif word in self.sentiment_dict['negative']:
                negative_count += 1
        
        total_sentiment_words = positive_count + negative_count
        if total_sentiment_words == 0:
            return 0.0
        
        # -1.0ï¼ˆå®Œå…¨ã«ãƒã‚¬ãƒ†ã‚£ãƒ–ï¼‰ã‹ã‚‰ 1.0ï¼ˆå®Œå…¨ã«ãƒã‚¸ãƒ†ã‚£ãƒ–ï¼‰
        sentiment_score = (positive_count - negative_count) / total_sentiment_words
        return sentiment_score
    
    def get_ngrams(self, n=2):
        """N-gramã‚’ç”Ÿæˆ"""
        if len(self.words) < n:
            return []
        
        ngrams = []
        for i in range(len(self.words) - n + 1):
            ngram = tuple(self.words[i:i + n])
            ngrams.append(ngram)
        
        return Counter(ngrams)
    
    def get_word_collocations(self, target_word, window=2):
        """æŒ‡å®šã—ãŸå˜èªã®å…±èµ·èªã‚’å–å¾—"""
        collocations = defaultdict(int)
        
        for i, word in enumerate(self.words):
            if word == target_word:
                # å‰å¾Œã®windowã‚µã‚¤ã‚ºåˆ†ã®å˜èªã‚’å–å¾—
                start = max(0, i - window)
                end = min(len(self.words), i + window + 1)
                
                for j in range(start, end):
                    if j != i:  # è‡ªåˆ†è‡ªèº«ã¯é™¤ã
                        collocations[self.words[j]] += 1
        
        return dict(collocations)
    
    def calculate_tf_idf(self):
        """TF-IDFé¢¨ã®ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—ï¼ˆç°¡æ˜“ç‰ˆï¼‰"""
        if not self.sentences:
            return {}
        
        # å„æ–‡ã‚’æ–‡æ›¸ã¨ã—ã¦æ‰±ã†
        documents = []
        for sentence in self.sentences:
            words = re.findall(r'\w+', sentence.lower())
            documents.append(words)
        
        if not documents:
            return {}
        
        # TFï¼ˆTerm Frequencyï¼‰è¨ˆç®—
        tf_scores = {}
        for word in self.word_count:
            tf = self.word_count[word] / len(self.words)
            
            # IDFï¼ˆInverse Document Frequencyï¼‰è¨ˆç®—
            doc_count = sum(1 for doc in documents if word in doc)
            idf = math.log(len(documents) / (doc_count + 1))
            
            tf_scores[word] = tf * idf
        
        return tf_scores
    
    def get_keyword_density(self, min_frequency=2):
        """ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å¯†åº¦ã‚’è¨ˆç®—"""
        if not self.words:
            return {}
        
        density = {}
        total_words = len(self.words)
        
        for word, count in self.word_count.items():
            if count >= min_frequency:
                density[word] = (count / total_words) * 100
        
        return density
    
    def analyze_word_patterns(self):
        """å˜èªãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’åˆ†æ"""
        patterns = {
            'starts_with_vowel': 0,
            'ends_with_consonant': 0,
            'contains_numbers': 0,
            'all_caps_words': 0,
            'repeated_letters': 0
        }
        
        vowels = set('aeiouAEIOU')
        consonants = set('bcdfghjklmnpqrstvwxyzBCDFGHJKLMNPQRSTVWXYZ')
        
        for word in self.words:
            # æ¯éŸ³ã§å§‹ã¾ã‚‹
            if word[0] in vowels:
                patterns['starts_with_vowel'] += 1
            
            # å­éŸ³ã§çµ‚ã‚ã‚‹
            if word[-1] in consonants:
                patterns['ends_with_consonant'] += 1
            
            # æ•°å­—ã‚’å«ã‚€
            if any(c.isdigit() for c in word):
                patterns['contains_numbers'] += 1
            
            # å…¨ã¦å¤§æ–‡å­—
            if word.isupper() and len(word) > 1:
                patterns['all_caps_words'] += 1
            
            # é€£ç¶šã™ã‚‹åŒã˜æ–‡å­—
            if any(word[i] == word[i+1] for i in range(len(word)-1)):
                patterns['repeated_letters'] += 1
        
        return patterns
    
    def export_analysis_to_json(self, filename="analysis_result.json"):
        """è§£æçµæœã‚’JSONã§å‡ºåŠ›"""
        try:
            result = {
                'basic_stats': self.stats,
                'top_words': self.word_count.most_common(20),
                'bigrams': list(self.get_ngrams(2).most_common(10)),
                'trigrams': list(self.get_ngrams(3).most_common(5)),
                'tf_idf_scores': dict(sorted(self.calculate_tf_idf().items(), key=lambda x: x[1], reverse=True)[:10]),
                'word_patterns': self.analyze_word_patterns(),
                'keyword_density': dict(sorted(self.get_keyword_density().items(), key=lambda x: x[1], reverse=True)[:10])
            }
            
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(result, f, indent=2, ensure_ascii=False)
            
            print(f"âœ… è§£æçµæœã‚’JSONã§ä¿å­˜ã—ã¾ã—ãŸ: {filename}")
            return True
            
        except Exception as e:
            print(f"âŒ JSONå‡ºåŠ›ã‚¨ãƒ©ãƒ¼: {e}")
            return False

def main():
    """å®Ÿè¡Œä¾‹"""
    analyzer = AdvancedWordAnalyzer()
    
    # ã‚µãƒ³ãƒ—ãƒ«ãƒ†ã‚­ã‚¹ãƒˆã§å‹•ä½œç¢ºèª
    sample_text = """
    This is a sample text for advanced word analysis. The text contains multiple sentences 
    with different word patterns. Some words are repeated, and some are unique. 
    We can analyze the sentiment, readability, and various linguistic features.
    This amazing tool provides excellent insights into text analysis.
    """
    
    analyzer.text = sample_text
    
    print("ğŸš€ é«˜åº¦ãªå˜èªè§£æãƒ‡ãƒ¢")
    print("=" * 50)
    
    # å‰å‡¦ç†
    analyzer.preprocess_text(remove_stopwords=True, language='english')
    
    # çµ±è¨ˆè¨ˆç®—
    analyzer.calculate_advanced_statistics()
    
    # çµæœè¡¨ç¤º
    print("\nğŸ“Š é«˜åº¦ãªçµ±è¨ˆæƒ…å ±:")
    for key, value in analyzer.stats.items():
        if isinstance(value, float):
            print(f"  {key}: {value:.3f}")
        else:
            print(f"  {key}: {value}")
    
    print("\nğŸ” é »å‡ºå˜èª:")
    for word, count in analyzer.word_count.most_common(5):
        print(f"  {word}: {count}å›")
    
    print("\nğŸ”— Bigrams:")
    bigrams = analyzer.get_ngrams(2)
    for bigram, count in bigrams.most_common(3):
        print(f"  {' '.join(bigram)}: {count}å›")
    
    print("\nğŸ¯ TF-IDFä¸Šä½:")
    tf_idf = analyzer.calculate_tf_idf()
    for word, score in sorted(tf_idf.items(), key=lambda x: x[1], reverse=True)[:3]:
        print(f"  {word}: {score:.3f}")
    
    # JSONå‡ºåŠ›
    analyzer.export_analysis_to_json("demo_analysis.json")

if __name__ == "__main__":
    main()

"""
ğŸ¯ å­¦ç¿’ã®ãƒã‚¤ãƒ³ãƒˆ:

1. é«˜åº¦ãªãƒ†ã‚­ã‚¹ãƒˆå‡¦ç†
   - ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰é™¤å»
   - N-gramè§£æ
   - å…±èµ·èªåˆ†æ

2. çµ±è¨ˆå­¦çš„æ‰‹æ³•
   - TF-IDFè¨ˆç®—
   - èª­ã¿ã‚„ã™ã•ã‚¹ã‚³ã‚¢
   - æ„Ÿæƒ…åˆ†æ

3. ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã®æ´»ç”¨
   - defaultdictã‚’ä½¿ã£ãŸé›†è¨ˆ
   - Counterã®é«˜åº¦ãªæ´»ç”¨
   - è¾æ›¸ã®å…¥ã‚Œå­æ§‹é€ 

4. ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®å®Ÿè£…
   - ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°
   - ã‚¹ã‚³ã‚¢è¨ˆç®—
   - ãƒ‡ãƒ¼ã‚¿å¤‰æ›å‡¦ç†

ğŸ”§ æ”¹è‰¯æ¡ˆ:
- ã‚ˆã‚Šé«˜åº¦ãªæ„Ÿæƒ…åˆ†æ
- è¨€èªæ¤œå‡ºæ©Ÿèƒ½
- é¡ä¼¼æ–‡æ›¸æ¤œç´¢
- è¦ç´„æ©Ÿèƒ½
- å¯è¦–åŒ–æ©Ÿèƒ½

âš¡ å®Ÿè¡Œæ–¹æ³•:
python3 projects/04_word_analyzer/advanced_analyzer.py
"""